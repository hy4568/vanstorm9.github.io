<html>

	<head>

		<link rel="stylesheet" href="css/bootstrap.min.css" type="text/css"/>
    <link href="css/cover.css" rel="stylesheet">
    <link href="personal.css" rel="stylesheet">


    <script type="text/javascript" src="https://ajax.googleapis.com/ajax/libs/jqueryui/1.8.11/jquery-ui.min.js"></script>
    
    <link rel="stylesheet" href="jquery/css/smoothness/jquery-ui-1.8.2.custom.css" /> 
    <script type="text/javascript" src="jquery/js/jquery-1.11.2.min.js"></script> 
    <script type="text/javascript" src="jquery/js/jquery-ui-1.8.2.custom.min.js"></script> 
    <script type="text/javascript" src="js/scroll.js"></script>
    

		
  


    </script>
	</head>
	
	<title>Anthony Lowhur</title>
	
	<body>
	
		<div class="site-wrapper" id="wrapper">

      <div class="site-wrapper-inner" id="inner-wrapper">

        <div class="cover-container" id="cover">

          <div class="masthead clearfix" id="mast">
            <div class="inner" id="inner">

              <h3 class="masthead-brand">Anthony Lowhur</h3>
              <nav>
                <ul class="nav masthead-nav">
                  <li class="active"><a href="#">Home</a></li>
                  <li><a id ="projectlink" href="#project">Projects</a></li>
                  <li><a href="Lowhur-Anthony-Resume.pdf">Resume</a></li>
                </ul>
              </nav>
            </div>
          </div>
          <br><br><br>
          <div class="inner cover">
            <div class="roundedImage">&nbsp;</div>
            <h1 class="cover-heading">Anthony Lowhur</h1>
            <p class="lead">Programmer, Hacker, Entrepreneur</p>
            <p class="lead">
              <a href="#project" class="btn btn-lg btn-default">Click to learn more</a>
            </p>

          </div>
          <br>
     

          <div class="mastfoot">
            <div class="inner">
              <p>Feel free to say hi at antlowhur@yahoo.com</p>
            </div>
          </div>

        </div>
        
      </div>


    </div>
<div id="project" style="background-color:#084B8A;">
  <br><br><br><br><br><br>
    <div id="project" style="background-color:#424242; width:80%;margin-right:10%;margin-left:10%;">
    
            <br><br>
            <h1>Programming Projects</h1>
            <br><br>
            <div class="roundedImageAI">&nbsp;</div>
            <font size="5">Artificial Intellegence</font>
            <br><br>
            <br><br>
            AI Melody Generator
            <br><br>
            <iframe width="560" height="315" src="https://www.youtube.com/embed/ImygJueWKQE" frameborder="0" allowfullscreen></iframe>
            <br><br>
            <center>
              <div style="width:70%">
                Created a melody generator that takes in a song from a midi, extracting musical aspects of song, recognizing patterns of the musical structure by using Long Short Term Memory(LSTM) Neural Network, and composing its own original melodies based on the patterns it had learned. Part of a long term project to create a full AI song composer, an AI that can generate entire songs with a series melodies.
                <br><br>
                

              </div>
            </center>

             <br><br>
            Dense Optical Flow based Emotion Recognition System
            <br><br>
            <iframe width="560" height="315" src="https://www.youtube.com/embed/pPclypFDcrk" frameborder="0" allowfullscreen></iframe>
            <br><br>
            <center>
              <div style="width:70%">
                Worked with a Rapiro robot by programming a computer vision program such as an emotion recognition system based on facemovements using Dense Optical Flow and trained it by using Support Vector Machines. Takes advantage of facial movements lack of vulnerability to lighting and unique facial appearance that an emotion classifier would havewith just static images, creating a more accurate and robust emotion classifier. 
                <br><br>
                Wrote a research paper and presented and published it as 1st author at the 2015 IEEE 12th International Conference (MASS) workshop in Dallas, Texas. 
                <br>
                <a href='http://ieeexplore.ieee.org/document/7366995/?reload=true&arnumber=7366995' target="_blank"  style="color: #2E9AFE">You can read it here</a>
                <br><br>
                (Note: the video above shows that the program shows the wrong confidence score information. This was fixed after the video was recorded.)
                <br><br>
                <a href="https://www.youtube.com/watch?v=PM_k_zACWLE" target="_blank"  style="color: #2E9AFE">Click here to see the program implemented in the Raspberry Pi of the Rapiro Robot</a>
              </div>
            </center>

            <br><br>
            Trap shooting tracker and hit detector 
            <br><br>
            <iframe width="560" height="315" src="https://www.youtube.com/embed/kFfQFqqoHSQ" frameborder="0" allowfullscreen></iframe>
            <br><br>
            <center>
              <div style="width:70%">
                Made a program in OpenCV that tracks a flying clay pigeon disk and recognize whether a bullet hit the disk or missed. Is also able to detect the different type of hits it made (whether it was a solid hit, a decent hit [it splits into fragments], or a weak hit [change trajectory]).
                <br><br>
                

              </div>
            </center>

            <br><br>
            Face Tracking via Haar Classification and Lucas Kanade
            <br><br>
            <iframe width="560" height="315" src="https://www.youtube.com/embed/42Snz1zzxUk" frameborder="0" allowfullscreen></iframe>
            <br><br>
            <center>
              <div style="width:70%">
                Improved face tracking sample from OpenCV documentation by combining both Harr Classifcation and Lucas Kanade optical flow algorithm. This allowed the computer to track the face/head in various different angles (front face to side face) even with limited face dataset
                <br><br>
                

              </div>
            </center>
        
            <br><br>
            Lyrics Generator Based on Markov Chains
            <br><br>
            <iframe width="560" height="315" src="https://www.youtube.com/embed/O0VlHfLAVzs" frameborder="0" allowfullscreen></iframe>
            <br><br>
            <center>
              <div style="width:70%">
                Implemented markov chains on lyrics data scrapped from the web to create program that can generate its own unique set of lyrics based on genre. Also have the capability of creating lyrics based on syllable count per line. Part of a long term project to create a full AI song composer. 
                <br><br>
                

              </div>
            </center>

            <br><br>
            Trash Detection on the Beach for Autonomous Drones though Sand Segmentation
            <br><br>
            <a href="https://github.com/nkoplitz/BeachUAS" target="_blank"><img src= "images/sand_segment.jpg" width="560" height="315"></a>
            <br><br>
            <center>
              <div style="width:70%">
                Developped a script that would allow an autonomous drone to analyze a beach scene, detect trash on the beach, analyzing it based on certain properties of the trash (such as shape),and pick it up accordingly. Implemented backhistogram projection and morphological transform in order to segment out the the sand and detect the trash. Used the Bag of Words model to preform object recognition on each of the trash pieces (recognize between trash paper, water bottle, or plastic bag). This will be able to autonomate the process of picking up trash on the beach.
                <br><br>
                

              </div>
            </center>


            <br><br>
            Pick up robot for Amazon Robotics Challenge
            <br><br>

            <img src= "images/amazon0.png" width="360" height="315"></a>
            <br>
            <img src= "images/amazon1.jpg" width="320" height="215"></a><img src= "images/amazon3.png" width="360" height="215"></a>
            <br><br>
            <iframe width="560" height="315" src="https://www.youtube.com/embed/v63rYOxZaDc" frameborder="0" allowfullscreen></iframe>

            <br><br>
            <center>
              <div style="width:70%">
                 Designeing intelligence of an Amazon Picking Challenge robot through computer vision and machine learning. Implemented 2D object recognition with convolution neural networks. Currently researching in effective image segmentation algorithms for object localization. Leading team of industrial engineering students (senior undergraduate + master graduates) for the development of the autonomous robot. Attempting to lead the team to be the first Rutgers team to enter nationals of the Amazon competition.
                <br><br>
                

              </div>
            </center>

            <br><br>
            Charm City Murals (mural detector)
            <br><br>
            
              
              <img src= "images/hophacks1.png" width="460" height="215"></a>     
              <img src= "images/hophacks2.png" width="460" height="215"></a>
              <br>
              <img src= "images/hophacks0.png" width="600" height="300"></a>
              

            <br><br>
            <center>
              <div style="width:70%">
                This program uses canny edge detection, morphological transform, and convolutional neural networks to detect and recognize historical murals on various walls in Baltimore. After it recognizes the mural, it would give information about the mural's history and origin. 
                <br><br>
                Won the 2nd place in HopHacks Fall 2018 hackathon, along with Google Cloud API and "Most-Baltimore" prize.
                <br><br>
                

              </div>
            </center>

            <br><br>
            Fire Detector and Analyzer
            <br><br>
            
             <iframe width="560" height="315" src="https://www.youtube.com/embed/IRmoD1BJFHM" frameborder="0" allowfullscreen></iframe>
            <br><br>
             <iframe width="560" height="315" src="https://www.youtube.com/embed/YIebUp8e3AA" frameborder="0" allowfullscreen></iframe>

            <br><br>
            <center>
              <div style="width:70%">
                This program uses computer vision searches for fires by searching for intense brightness and fire-like movement in the video. It then monitors the rate of growth and current size of the fire to determine on how threatening a fire is becoming and notifies the user if it gets to a certain size / surpasses a certain growth rate.


                <br><br>
                This was designed for PennApps Fall 2018 where it placed in top 30 and won the DocuSign API prize.
                <br><br>
                

              </div>
            </center>


          <br><br>
            Hand-drawn graphs to LATEX translator
            <br><br>
            
              <a href="https://github.com/aliang8/Paper2LaTeX" target="_blank"><img src= "images/graph0.png" width="340" height="200"><img src= "images/graph1.png" width="340" height="200"></a>

            <br><br>
            <center>
              <div style="width:70%">
                Implented computer vision and machine learning in order to analyze a hand-drawn graph on paper, seach and analyze shapes such as nodes and lines, and translate them into a more tidy and neat LATEX graph. Helped plan computer vision algorithms to detect nodes and lines while worked on optical character recognitiion and arrow direction detection.
                <br><br>
                Placed in top 30 hacks in PennApps Winter 2017 hackathon.
                <br><br>
                

              </div>
            </center>

             <br><br>
            CNN-LSTM Video Classifier 
            <br><br>
            
              <iframe width="560" height="315" src="https://www.youtube.com/embed/yzEKe9Eoa0Q" frameborder="0" allowfullscreen></iframe>

            <br><br>
            <center>
              <div style="width:70%">
               This video was a result of training a CNN-LSTM video classifer on a small dataset. By analyzing a series of frames in each video, the classifer is able to recognize actions going on in the video. 

		By having CNN encoder taking in the frames, the frames are parsed to be analyzed by the LSTM which analyzes the temporial structure of the frames and decodes the output to get a classification of the video.  

              </div>
            </center>



             <br><br>
            Video Panorama Generator 
            <br><br>
            
              <iframe width="560" height="315" src="https://www.youtube.com/embed/hVynpc-li1w" frameborder="0" allowfullscreen></iframe>

            <br><br>
            <center>
              <div style="width:70%">
                This video is a result from a python script I made that stitches two videos that were recorded from two cameras looking at the same scene at different angle to create video panoramas. Uses the SIFT algorithm to find similar points and calculate a transformation matrix with OpenCV to stitch the videos to together
                This video is the end result. May work on improving making the seams less visible when I get the time.
                

              </div>
            </center>


            <br><br>
            Surround and Capturing Adversarial Agents through Decentralized Multi-Agent Intelligence
            <br><br>
            
              <a href="TTUResearch0.pdf"><img src= "images/swarm0.png" width="440" height="340"></a>

            <br><br>
            <center>
              <div style="width:70%">
                Designed a multi-agent intelligence algorithm (swarm intelligence) where a team of ally agents work together to surround and capture a fleeing adversarial agent. This allowed agents to search for interest points to pursue and surround enemy agent with individual behavior, but minimal communication to work together as an agent team for capturing an adversarial agent.
                <br><br>
                Abstract was accepted to the National Conference On Undergraduate Research (NCUR 2017) at the Memphis, Tennessee.
                <br><br>
                <a href="TTUResearch0.pdf" target="_blank" style="color: #2E9AFE">You can see the research poster here</a>
                <br><br>
                

              </div>
            </center>



            <br><br>
            Road Segmentation For Autonomous Vehicles
            <br><br>
            <a href="https://github.com/vanstorm9/urban-road-recognition" target="_blank"><img src= "images/road_0.png" width="560" height="270"></a>
            <br><br>
            <center>
              <div style="width:70%">
                Designed a simple script that uses histogram backprojection along with morphological transform in order detect a road in a scene despite any large amounts of noise in the picture. Made to be implemented in the DriveAI project, an initiative for open-source autonomous vehicles.
                <br><br>
                

              </div>
            </center>

            <br><br>
            Rutgers Navigation System
            <br><br>
            <iframe width="560" height="315" src="https://www.youtube.com/embed/Pt3fnORMzA8" frameborder="0" allowfullscreen></iframe>
            <br><br>
            <center>
              <div style="width:70%">
                Uses the A* algorithm in order to find the shortest path from point A to point B within Rutgers campus (to find buildings, bus stops, dining halls, etc) (currently Busch campus) as well as kinematics equations to find the time of the walk. Calcuates both the path and information of bus route and walking route to help one determine the more efficient way of getting to a destination (takes into account the bus route, average walking and bus driving speed, and distance between each places)
                <br><br>
                

              </div>
            </center>


            <br><br><br><br><br><br><br><br><br><br><br><br>
            <div class="roundedImageHCI">&nbsp;</div>
            <font size="5">Human-Computer Interaction</font>
            <br><br>
            <br><br>
            Force VR Gauntlet
            <br><br>
            <iframe width="560" height="315" src="https://www.youtube.com/embed/PMwATLDO3yU" frameborder="0" allowfullscreen></iframe>
            <br><br>
            <center>
              <div style="width:70%">
                Used a motor (with rope tethered to arm) controlled by the Arduino to make a virtual reality wearable that allows the wearer to experience physical forces. Whenever the user moves / swings his arm in the VR game and a hit is detected, the Arduino recieves that information and pulls on the arm with the motor tethered to arm, creating a locking effect and imitation of a physical force. It was designed to solve the problem of lack of physical forces in virtual reality (say you have a sword. You hit an object and your arm stops because it cannot go through an object, but in VR, your arm would simply phase through the object). With this developped a little bit more, actual sword combat will be finally possible in virtual reality.
                <br><br>
              </div>
            </center>

            <br><br>
            Kinect Helper
            <br><br>
            <iframe width="560" height="315" src="https://www.youtube.com/embed/svzljeO0i2k" frameborder="0" allowfullscreen></iframe>
            <br><br>
            <center>
              <div style="width:70%">
                Used the Kinect to create a workspace that is able to sense the presence of its owner. By using object detection, the
                Kinect is able to call a bash script that gives the Kinect control of the computer based on user input, creating a smart workspace. In this case the Kinect can sense when you get up from your chair and will automatically shut off the screen, this saves power and is more convenient than having to do it manually. We also included other features such as the ability to change the brightness of the screen using a hand held up to the sensor. Any other features can be added to the program and controlled with a hand held up to the Kinect.
                <br><br>
                
              </div>
            </center>

            <br><br>
            Leaptop
            <br><br>
            <iframe width="560" height="315" src="https://www.youtube.com/embed/SqsctnDnpdc" frameborder="0" allowfullscreen></iframe>
            <br><br>
            <center>
              <div style="width:70%">
                Used the Leap Motion in order to control some computer functions using just hand gestures. This includes opening up a browser, closing a browser, putting the laptop to sleep, and locking it using hand movements. This was made in Python which was also controlling a bash script.
                <br><br>
              </div>
            </center>
            <br><br>
            Hologram Pyramid CAD program
            <br><br>

            <iframe width="560" height="315" src="https://www.youtube.com/embed/Kmb2xlh2wm8" frameborder="0" allowfullscreen></iframe><br>
            <iframe width="560" height="315" src="https://www.youtube.com/embed/1y1NO4AM0k4" frameborder="0" allowfullscreen></iframe>

            <br><br>
            <center>
              <div style="width:70%">
                
                EDIT: I know, the quality of the video isn't that good, so I will upload another video in the near future. <br><br>

              A friend and I made a projector that uses pepper's ghost to create a hologram like image in an acrylic pyramid. Used Unity for the models and projection scene along with Leap Motion to give us the ability to control the orientation of the model. If developped more, it would serve as a CAD program for designers, artists, and engineers so that they can view their models and control them in 3D space.
                <br><br>
              </div>
            </center>
            <br><br>
            Hologram Emotion Face Control
            <br><br>

            <a href="https://github.com/megatran/EmotionRecognitionOnHologram" target="_blank"><img src= "images/emotionPyramid.jpg" width="560" height="315"></a>

            <br><br>
            <center>
              <div style="width:70%">
                
              We use the hologram pyramid to project a face into the pyramid. By using convolution neural networks and a server, we were able to change the the hologram face's emotion from happy to angry depending on what the emotion of someone else on another computer. We hope to work on full face control in the future.
                <br><br>
              </div>
            </center>
            <br><br>
            <font size="5">Game Development</font>
            <br><br>

            Myo Multiplayer Sword Fighting Game
            <br><br>
            <iframe width="560" height="315" src="https://www.youtube.com/embed/6phNDQvRKuU" frameborder="0" allowfullscreen></iframe>
            <br><br>
            <center>
              <div style="width:70%">
                Created a sword fighting multiplayer game wheremultiple people can log into the same server and engage sword combat with each other. Programmedthe Myo so that users are able to preform sword attacks through sword cutting like hand gestures. It was designed as a test game that may be implemented with the Force VR Gauntlet.
                <br><br>
              </div>
            </center>
            <br><br>
            Twitch Based Jenga Multiplayer Game
            <br><br>
            <iframe width="560" height="315" src="https://www.youtube.com/embed/v_voaCJqpFA" frameborder="0" allowfullscreen></iframe>
            <br><br>
            <center>
              <div style="width:70%">
                Created a Twitch based collaboration game in Jenga where players can log into the same server and play Jenga with each other while the audience is able to interfer with the gameplay in Unity by tying in commands on Twitch. Created this as my first Unity game development project.
                <br><br>
              </div>
            </center>
             <br><br>
            NES Zelda recreaton
            <br><br>
            <iframe width="560" height="315" src="https://www.youtube.com/embed/StfCZy4-NRA" frameborder="0" allowfullscreen></iframe>
            <br><br>
            <center>
              <div style="width:70%">
                A game I made in Visual Basic, which is a small segment recreation of the Legend of Zelda for NES.
              </div>

            <br><br>
            <font size="5">Web Development</font>
            
            <br><br>
            Love Live Waifu Simulator
            <br><br>
            <a href="https://llsif-waifu-sim.github.io/" target="_blank"><img src= "images/llsifwaifu.png" width="560" height="315"></a>
            
            <center>
              <div style="width:70%">
                A site dedicated for the show, "Love Live", where fans can click on their favorite characters to get them to speak, listen to music, and watch videos. People can change their character dynamically, get screenshots, and discuss their fandom with other people on a chatroom.
                <br><br>
                <a href="https://llsif-waifu-sim.github.io/" target="_blank"  style="color: #2E9AFE">Check out the site</a>
              </div>
            <br><br>
            WannaStudy
            <br><br>
            <center>
              <div style="width:70%">
                A social networking forum designed to help create academic community and supports the circulation of academic information within colleges in order for students to ask and answer questions, form study groups, study online through Google Hangouts (button integrated on site), and share information.
              </div>
            <br><br>
            Rutgers Roommate Search Engine
            <br><br>
            <center>
              <div style="width:70%">
                Account system website designed to have users register themselves to a roommate search engine in order to help them or others to find the perfect roommate. Includes an advanced search section, a list of roommates filtered through advance search, profile pages for each user, and roommate lists to add potential roommates on their list. 
                <br><br>
                <a href="http://rutgersroommate.site11.com" target="_blank"  style="color: #2E9AFE">Check out the site</a>
              </div>
            <br><br>
            PVHS GPA Calculator
            <br><br>
            <iframe width="560" height="315" src="https://www.youtube.com/embed/to74NTcSUtE" frameborder="0" allowfullscreen></iframe>
            <br><br>
            <center>
              <div style="width:70%">
                An old website with PHP/SQL account systems that helps students calculate their GPA (marking period, end-of-year, and current)as well final grad. 
              </div>
            <br><br>


            <br><br><br><br><br><br><br><br><br><br>
          </div>
	 </div>
	
		<script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.0/jquery.min.js"></script>
		<script src = "js/bootstrap.js"></script>

	</body>


	
</html>
